---
title: "Classification Analysis"
author: "Le Hai Trieu Tran"
date: "`r Sys.Date()`"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1.
1.	Model comparison (20 points) – include the code used
Using the own data develop two classification models (different kinds) and compare them. 
A.	Make sure the data set is balanced

B.	Use two different sizes of the data set.  The sizes should be different by a magnitude of at least two times.  For instance, two sizes – 50 and 100.
Generate a training set and a test set for each size.

C.	Run both classifiers for the data.  You should have eight results -Two for each classifier times two for each size times two for the train and test set.

D.	Compare the result of the different classifiers. Which classifier model would you use and why?

E.	Using bagging and boosting and the same data sets carry out the same  analysis as above. 

F.	Compare the results of classifiers and the bagging/boosting results.


```{r message=FALSE}
#load data
library(readr)
library(dplyr)
library(forecast)
bank <- read_csv("/Users/trieutlh/Library/CloudStorage/OneDrive-UW/Archive/TBANLT 560/final exam/data/bank_churn.csv")
head(bank)
summary(bank)
```
### A. Make sure the data set is balanced – you will want to do that theself. Describe and justify the method for balancing the data set.  
``` {r message = FALSE}
#initial look at the data
table(bank$churn)
bank$churn <- as.factor(bank$churn)
library(ggplot2)
g <- ggplot(bank, aes(fill = churn)) + geom_bar(aes(x = churn))
g
file_path <- "/Users/trieutlh/Library/CloudStorage/OneDrive-UW/Archive/TBANLT 560/final exam/figures/q1"
ggsave(file=file.path(file_path, "q1_g.png"),
       g, width=5, height=8, units="in")
``` 
This data has about 20% of Minority Class (churn = 1), so it is an imbalanced dataset. I will use ROSE package to undersampling the data.  *Describe the process:*     

*	ROSE Package Installation and Loading: The ROSE package is installed using the install.packages() function and loaded into R with library(ROSE).
*	Determining Sampling Fraction: The total number of samples in the dataset is calculated, and a new sampling fraction (new_frac) is defined, which is set to 0.5 in this case.
*	Combining Undersampling and Oversampling: ovun.sample() function from the ROSE package is utilized to perform both undersampling and oversampling.
*	Sampling Result: The operation is executed, resulting in a sampled dataset (sampled_bank), which now has a balanced distribution of classes.
*	Visualization: A bar plot is created using ggplot() to visualize the distribution of the churn variable in the sampled dataset after sampling.  

``` {r message = FALSE}
#Installing and Loading ROSE Package
if(!require(ROSE)) install.packages("ROSE")
library(ROSE)

#Determining Sampling Fraction 
n_new <- nrow(bank)
new_frac <- 0.5

#Combine both Undersampling and Oversampling
sampling_result <- ovun.sample(formula = churn ~., data = bank,
                                 method = "both", 
                                 N = n_new,
                                 p = new_frac,
                                 seed = 10)

sampled_bank <- sampling_result$data
table(sampled_bank$churn)
g2 <- ggplot(sampled_bank, aes(fill = churn)) + geom_bar(aes(x = churn))
g2
ggsave(file=file.path(file_path, "q1_g2.png"),
       g2, width=5, height=8, units="in")
```
This is the result data after undersampling.  

### B. Use two different sizes of the data set. The sizes should be different by a magnitude of at least two times.  For instance, two sizes – 50 and 100.  
Generate a training set and a test set for each size. (There will be four sets total.)  
``` {r}
#Change numerical variables to categorical
sampled_bank$churn <- as.factor(sampled_bank$churn)
sampled_bank$products_number <- as.factor(sampled_bank$products_number)
sampled_bank$active_member <- as.factor(sampled_bank$active_member)
sampled_bank$credit_card <- as.factor(sampled_bank$credit_card)

#Normalize data
library(caret)
norm_values <- preProcess(sampled_bank, method = "scale")
bank_data1 <- predict(norm_values, sampled_bank)
bank_data1 <- bank_data1[-1] #customer_id excluded

#Partition data 
set.seed(10)
index <- sample(row.names(bank_data1), 0.5*dim(bank_data1)[1]) 
bank_data2 <- bank_data1[index, ]

#bank_data1: Generate a training set and a test set
set.seed(10)
train.index_data1 <- sample(c(1:dim(bank_data1)[1]), dim(bank_data1)[1]*0.6) 
train_data1 <- bank_data1[train.index_data1, ] 
valid_data1 <- bank_data1[-train.index_data1, ]

#bank_data2: Generate a training set and a test set
set.seed(10)
train.index_data2 <- sample(c(1:dim(bank_data2)[1]), dim(bank_data2)[1]*0.6) 
train_data2 <- bank_data2[train.index_data2, ] 
valid_data2 <- bank_data2[-train.index_data2, ]
```

### C.	Run both classifiers for the data. You should have eight results -Two for each classifier times two for each size times two for the train and test set.  
``` {r}
#Model 1: Naive Bayes using train_data1 and valid_data1
library(e1071)

#Run Naive Bayes 
bank_data1_nb <- naiveBayes(churn ~ ., data = train_data1)
bank_data1_nb
```
```{r}
#Predict on train_data1
#predict probabilities
pred.prob <- predict(bank_data1_nb, newdata=train_data1, type="raw")
#predict class membership
pred.class <- predict(bank_data1_nb, newdata=train_data1)

df <- data.frame(actual = train_data1$churn, predicted = pred.class, pred.prob)
head(df)
```
``` {r}
#result 1
#train_data1 
#accuracy
accuracy(as.numeric(pred.class), as.numeric(train_data1$churn))
#Confusion matrix
pred.class <- predict(bank_data1_nb, newdata = train_data1) 
confusionMatrix(pred.class, train_data1$churn)
```

```{r}
#Predict on valid_data1
## predict probabilities
pred.prob <- predict(bank_data1_nb, newdata=valid_data1, type="raw")
## predict class membership
pred.class <- predict(bank_data1_nb, newdata=valid_data1)

df <- data.frame(actual = valid_data1$churn, predicted = pred.class, pred.prob)
head(df)
```


``` {r}
#result 2
#valid_data1 
#accuracy
accuracy(as.numeric(pred.class), as.numeric(valid_data1$churn))
#Confusion matrix
pred.class <- predict(bank_data1_nb, newdata = valid_data1) 
confusionMatrix(pred.class, valid_data1$churn)
```


``` {r}
#Model 1: Naive Bayes using train_data2 and valid_data2
library(e1071)

#Run Naive Bayes 
bank_data2_nb <- naiveBayes(churn ~ ., data = train_data2)
bank_data2_nb
```
```{r}
#Predict on train_data2
#predict probabilities
pred.prob <- predict(bank_data2_nb, newdata=train_data2, type="raw")
#predict class membership
pred.class <- predict(bank_data2_nb, newdata=train_data2)

df <- data.frame(actual = train_data2$churn, predicted = pred.class, pred.prob)
head(df)
```

``` {r}
#result 3
#train_data2
#accuracy
accuracy(as.numeric(pred.class), as.numeric(train_data2$churn))
#Confusion matrix
pred.class <- predict(bank_data2_nb, newdata = train_data2) 
confusionMatrix(pred.class, train_data2$churn)
```

```{r}
#Predict on valid_data2
#predict probabilities
pred.prob <- predict(bank_data2_nb, newdata=valid_data2, type="raw")
#predict class membership
pred.class <- predict(bank_data2_nb, newdata=valid_data2)

df <- data.frame(actual = valid_data2$churn, predicted = pred.class, pred.prob)
head(df)
```


```{r}
#result 4
#valid_data2
#accuracy
accuracy(as.numeric(pred.class), as.numeric(valid_data2$churn))
#Confusion matrix
pred.class <- predict(bank_data2_nb, newdata = valid_data2) 
confusionMatrix(pred.class, valid_data2$churn)

```


``` {r message = FALSE}
# Model 2: Logistic regression using train_data1 and valid_data1
# run logistic regression # use glm() (general linear model) with family = "binomial" to fit a logistic # regression.
options(scipen=999)
model2 <- glm(churn ~ ., data = train_data1, family = "binomial")  
model2

#predict on train_data1
model2.pred.train <- predict(model2, train_data1, type = "response")
model2.preb.train.binary <- ifelse(model2.pred.train>0.5,1,0)
#predict on valid_data1
model2.pred.valid <- predict(model2, valid_data1, type = "response")
model2.preb.valid.binary <- ifelse(model2.pred.valid>0.5,1,0)
```

``` {r}
#result 5
#train_data1
#accuracy
accuracy(model2.preb.train.binary, as.numeric(train_data1$churn))
#Confusion matrix
model2.pred <- predict(model2, train_data1, type = "response")
confusionMatrix(as.factor(ifelse(model2.pred > 0.5, 1, 0)), 
                train_data1$churn)
```
``` {r}
#result 6
#valid_data1
#accuracy
accuracy(model2.preb.valid.binary, as.numeric(valid_data1$churn))
#Confusion matrix
model2.pred <- predict(model2, valid_data1, type = "response")
confusionMatrix(as.factor(ifelse(model2.pred > 0.5, 1, 0)), 
                valid_data1$churn)
```

``` {r warning = FALSE}
# Model 2: Logistic regression using train_data2 and valid_data2
# run logistic regression # use glm() (general linear model) with family = "binomial" to fit a logistic # regression.
options(scipen=999)
model2 <- glm(churn ~ ., data = train_data2, family = "binomial")  
model2

#predict on train_data2
model2.pred <- predict(model2, train_data2, type = "response")
model2.preb.binary <- ifelse(model2.pred>0.5,1,0)

#predict on valid_data2
model2.pred.valid <- predict(model2, valid_data2, type = "response")
model2.preb.valid.binary <- ifelse(model2.pred.valid>0.5,1,0)
```

``` {r}
#result 7
#train_data2
#accuracy
accuracy(model2.preb.binary, as.numeric(train_data2$churn))
#Confusion matrix
model2.pred <- predict(model2, train_data2, type = "response")
confusionMatrix(as.factor(ifelse(model2.pred > 0.5, 1, 0)), 
                train_data2$churn)
```

``` {r}
#result 8
#valid_data2
#accuracy
accuracy(model2.preb.binary, as.numeric(valid_data2$churn))
#Confusion matrix
model2.pred <- predict(model2, valid_data2, type = "response")
confusionMatrix(as.factor(ifelse(model2.pred > 0.5, 1, 0)), 
                valid_data2$churn)
```

### D. Compare the result of the different classifiers. Which classifier model would you use and why?  
Compare the result of the different classifiers:  

*	Accuracy: Both classifiers seem to perform similarly in terms of accuracy, with Naïve Bayes slightly outperforming Logistic Regression on some datasets.  
*	ME, RMSE, and MAE: Naïve Bayes generally shows lower error metrics (ME, RMSE, MAE) compared to Logistic Regression, indicating better performance in terms of prediction accuracy.  
*	MPE and MAPE: Naïve Bayes also shows lower MPE and MAPE values, suggesting better accuracy in predicting the target variable (churn).  
Conclusion: In this case, we should use Naïve Bayes, because it exhibits lower error rates and better MPE and MAPE compared to Logistic Regression.  

### E. Using bagging and boosting   
``` {r message = FALSE}
if(!require(adabag)) install.packages("adabag")
library(adabag) 
library(rpart)
```

```{r}
# bagging 
bag <- bagging(churn ~ ., data = train_data1)
pred <- predict(bag, valid_data1, type = "class") 
```
```{r}
# boosting 
boost <- boosting(churn ~ ., data = train_data1) 
pred <- predict(boost, valid_data1, type = "class") 
```

### F.	Compare the results of classifiers and the bagging/boosting results.  Put the results here and discuss the results.
```{r}
# bagging result
bag <- bagging(churn ~ ., data = train_data1)
pred <- predict(bag, valid_data1, type = "class") 
accuracy(as.numeric(pred$class), as.numeric(valid_data1$churn))
confusionMatrix(as.factor(pred$class), valid_data1$churn)
```
```{r}
# boosting result
boost <- boosting(churn ~ ., data = train_data1) 
pred <- predict(boost, valid_data1, type = "class") 
accuracy(as.numeric(pred$class), as.numeric(valid_data1$churn))
confusionMatrix(as.factor(pred$class), valid_data1$churn)
```

*Compare the results of classifiers and the bagging/boosting results:*  
* Accuracy: Boosting has the highest accuracy among all methods.  
* ME, RMSE, and MAE: Boosting and Bagging generally have lower error metrics compared to Naïve Bayes and Logistic Regression, indicating better performance in terms of prediction accuracy.  
* MPE and MAPE: Boosting and Bagging also show lower MPE and MAPE values, suggesting better accuracy in predicting the target variable compared to Naïve Bayes and Logistic Regression.  
Conclusion:  
Boosting seems to be the best performing model among all the classifiers in this case, with the highest accuracy and lower error metrics. 
Bagging also performs well but slightly less than boosting.  
Naïve Bayes and Logistic Regression do not perform as well as Bagging and Boosting in this scenario.  

